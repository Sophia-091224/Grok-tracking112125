# app.py - é£Ÿå“æº¯æºAIç³»çµ± v2.0 - Hugging Face Spaces ä¸€éµéƒ¨ç½²ç‰ˆ
# 2025-11-21 å®Œå…¨å–®æª”ç‰ˆæœ¬ï¼ˆå…§å»º agents.yaml + 31å€‹ä»£ç†é‚è¼¯ï¼‰

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import yaml
import os
from typing import Dict, Any
import time

# ==================== å…§å»º agents.yaml ====================
AGENTS_CONFIG = yaml.safe_load('''
agent_031:
  name: "ç¸½å”èª¿å“¡ - Food Traceability Orchestrator"
  role: orchestrator
  model: gpt-4o
  description: "çµ±ç±Œ31å€‹å°ˆæ¥­AIä»£ç†ï¼ŒæŒ‰éšŽæ®µè‡ªå‹•åŸ·è¡Œ"

data_cleaning:
  agent_001: { name: "æ•¸æ“šçµæ§‹åˆ†æžå¸«", role: "æª¢æŸ¥æ¬„ä½ã€è³‡æ–™åž‹æ…‹ã€å”¯ä¸€æ€§" }
  agent_002: { name: "ç¼ºå¤±å€¼è¨ºæ–·å°ˆå®¶", role: "è­˜åˆ¥ä¸¦å»ºè­°å¡«è£œç­–ç•¥" }
  agent_003: { name: "ç•°å¸¸å€¼åµæ¸¬å“¡", role: "åŸºæ–¼3Ïƒèˆ‡ç®±å½¢åœ–æª¢æ¸¬""
  }
  agent_004: { name: "æ—¥æœŸæ ¼å¼çµ±ä¸€å¸«", role: "è§£æžä¸¦æ¨™æº–åŒ–æ‰€æœ‰æ—¥æœŸæ¬„ä½" }
  agent_005: { name: "æº«åº¦è¨˜éŒ„é©—è­‰å¸«", role: "æª¢æŸ¥å†·éˆæº«åº¦æ˜¯å¦ç¬¦åˆ2-8Â°C" }
  agent_006: { name: "æ‰¹æ¬¡IDä¸€è‡´æ€§æª¢æŸ¥å“¡", role: "ç¢ºä¿æ‰¹æ¬¡IDåœ¨å„éšŽæ®µä¸€è‡´" }

statistical:
  agent_007: { name: "æè¿°æ€§çµ±è¨ˆåˆ†æžå¸«", role: "è¨ˆç®—å¹³å‡ã€æ¨™æº–å·®ã€åˆ†ä½ˆ" }
  agent_008: { name: "æ™‚é–“åºåˆ—åˆ†æžå¸«", role: "ç”¢è›‹â†’åŒ…è£â†’é‹è¼¸æ™‚é–“é–“éš”åˆ†æž" }
  agent_009: { name: "è¾²å ´ç¸¾æ•ˆæ¯”è¼ƒå°ˆå®¶", role: "è·¨è¾²å ´KPIæ¯”è¼ƒ" }

visualization:
  agent_014: { name: "å„€è¡¨æ¿ç¸½è¨­è¨ˆå¸«", role: "è¨­è¨ˆæ•´é«”Dashboardå¸ƒå±€" }
  agent_015: { name: "å†·éˆæº«åº¦ç†±åœ–å°ˆå®¶", role: "ç”Ÿæˆæ™‚é–“ vs æº«åº¦ç†±åœ–" }

risk_assessment:
  agent_021: { name: "HACCPé¢¨éšªè©•åˆ†ç¸½å¸«", role: "è¨ˆç®—ç¶œåˆé¢¨éšªåˆ†æ•¸ï¼ˆ0-10ï¼‰" }
  agent_022: { name: "å†·éˆä¸­æ–·åµæ¸¬å“¡", role: "æº«åº¦>8Â°Cè¶…éŽ2å°æ™‚å³æ¨™è¨˜é«˜é¢¨éšª" }

ai_enhanced:
  agent_027: { name: "è‡ªç„¶èªžè¨€æŸ¥è©¢å¼•æ“Ž", role: "æ”¯æ´ä¸­/è‹±/æ—¥å•ç­”" }
  agent_031: { name: "æœ€çµ‚å ±å‘Šç”Ÿæˆç¸½ç›£", role: "å½™æ•´æ‰€æœ‰ä»£ç†è¼¸å‡ºï¼Œç”¢å‡ºPDFç´šå ±å‘Š" }
''')

# ==================== ç³»çµ± Promptï¼ˆä¾†è‡ªè¦æ ¼ç¬¬4ç« ï¼‰ ====================
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å°ç£é£Ÿå“æº¯æºèˆ‡å®‰å…¨AIå°ˆå®¶ï¼Œå°ˆæ³¨æ–¼é›žè›‹å†·éˆè¿½æº¯ã€‚
é—œéµæ³•è¦èˆ‡æ¨™æº–ï¼š
- å†·è—æº«åº¦å¿…é ˆä¿æŒåœ¨ 2~8Â°C
- ç”¢è›‹åˆ°åŒ…è£ä¸å¾—è¶…éŽ 24 å°æ™‚
- æ´—é¸è›‹ä¿å­˜æœŸé™æœ€å¤š 28 å¤©
- å†·éˆä¸­æ–·è¶…éŽ 2 å°æ™‚è¦–ç‚ºé«˜é¢¨éšª

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›žè¦†ï¼Œè¼¸å‡ºæ ¼å¼ï¼š
# âœ¨ æœ€çµ‚å ±å‘Š

## âš ï¸ é¢¨éšªç¸½è©•


## ðŸ“Š é—œéµç™¼ç¾


## ðŸ”§ å»ºè­°è¡Œå‹•


## ðŸ“ˆ è¦–è¦ºåŒ–åœ–è¡¨
ï¼ˆåœ¨æ­¤æè¿°åœ–è¡¨å…§å®¹ï¼‰

åš´æ ¼éµå®ˆï¼šä¸å½é€ æ•¸æ“šã€ä¸æä¾›æ³•å¾‹å»ºè­°ã€æ‰€æœ‰é«˜é¢¨éšªå¿…é ˆæ¨™è¨»ä¾†æºã€‚
"""

# ==================== ç°¡åŒ–ç‰ˆ LLM å‘¼å«ï¼ˆæ”¯æ´ OpenAI / Gemini / Grokï¼‰ ====================
@st.cache_resource
def get_llm_client():
    openai_key = st.session_state.get("openai_key", "")
    gemini_key = st.session_state.get("gemini_key", "")
    groq_key = st.session_state.get("groq_key", "")

    try:
        import openai
        if openai_key and openai_key.startswith("sk-"):
            client = openai.OpenAI(api_key=openai_key)
            return lambda prompt, model: client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": SYSTEM_PROMPT},
                          {"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=3000
            ).choices[0].message.content
    except: pass

    try:
        import google.generativeai as genai
        if gemini_key:
            genai.configure(api_key=gemini_key)
            model = genai.GenerativeModel('gemini-1.5-pro')
            return lambda prompt, _: model.generate_content(SYSTEM_PROMPT + prompt).text
    except: pass

    try:
        from groq import Groq
        if groq_key:
            client = Groq(api_key=groq_key)
            return lambda prompt, model: client.chat.completions.create(
                model="llama3-70b-8192" if "70b" in model else "llama3-8b-8192",
                messages=[{"role": "system", "content": SYSTEM_PROMPT},
                          {"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=3000
            ).choices[0].message.content
    except: pass

    return None

# ==================== ä»£ç†æ¨¡æ“¬åŸ·è¡Œï¼ˆ31å€‹ä»£ç†æ ¸å¿ƒé‚è¼¯ï¼‰ ====================
def run_all_agents(df: pd.DataFrame, llm_call, model: str) -> Dict[str, Any]:
    progress = st.progress(0)
    status = st.empty()
    results = {"notes": [], "figures": {}}

    # Agent 001-006: æ•¸æ“šæ¸…ç†
    status.text("ðŸ§¹ Agent 001-006ï¼šæ•¸æ“šæ¸…ç†èˆ‡é©—è­‰ä¸­...")
    progress.progress(10)

    # è‡ªå‹•æ—¥æœŸè§£æž
    date_cols = ["laying_date", "packing_date", "distribution_date", "ç”¢è›‹æ—¥æœŸ", "åŒ…è£æ—¥æœŸ", "å‡ºè²¨æ—¥æœŸ"]
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')

    # æº«åº¦æ¬„ä½çµ±ä¸€è™•ç†
    temp_cols = [c for c in df.columns if any(k in c.lower() for k in ["temp", "æº«åº¦"])]
    if temp_cols:
        df["temperature_violation"] = df[temp_cols[0]].apply(lambda x: x > 8 or x < 2 if pd.notna(x) else False)

    results["notes"].append("âœ… æ•¸æ“šçµæ§‹å·²æ¨™æº–åŒ–ï¼Œæº«åº¦æ¬„ä½å·²é©—è­‰")

    # Agent 007-013: çµ±è¨ˆåˆ†æž
    status.text("ðŸ“Š Agent 007-013ï¼šçµ±è¨ˆåˆ†æžä¸­...")
    progress.progress(40)
    time.sleep(1)

    stats = {
        "ç¸½æ‰¹æ¬¡æ•¸": len(df),
        "å¹³å‡æº«åº¦": df[temp_cols[0]].mean() if temp_cols else None,
        "æº«åº¦ç•°å¸¸æ‰¹æ¬¡": df["temperature_violation"].sum() if "temperature_violation" in df.columns else 0,
        "é«˜é¢¨éšªæ‰¹æ¬¡": df[df["temperature_violation"] == True]["batch_id"].tolist() if "bath_id" in df.columns else []
    }
    results["notes"].append(f"ðŸ”¢ ç™¼ç¾ {stats['æº«åº¦ç•°å¸¸æ‰¹æ¬¡']} å€‹æº«åº¦ç•°å¸¸æ‰¹æ¬¡")

    # Agent 014-020: å¯è¦–åŒ–
    status.text("ðŸŽ¨ Agent 014-020ï¼šç”Ÿæˆåœ–è¡¨ä¸­...")
    progress.progress(70)
    time.sleep(1)

    if temp_cols and "laying_date" in df.columns:
        fig1 = px.line(df, x="laying_date", y=temp_cols[0], color="batch_id" if "batch_id" in df.columns else None,
                       title="ðŸ” å†·éˆæº«åº¦è¶¨å‹¢åœ–ï¼ˆ2-8Â°C ç‚ºå®‰å…¨ç¯„åœï¼‰")
        fig1.add_hline(y=8, line_dash="dash", line_color="red", annotation_text="å±éšªä¸Šé™ 8Â°C")
        fig1.add_hline(y=2, line_dash="dash", line_color="blue", annotation_text="å±éšªä¸‹é™ 2Â°C")
        results["figures"]["æº«åº¦è¶¨å‹¢"] = fig1

    # Agent 021-026: é¢¨éšªè©•ä¼°
    status.text("âš ï¸ Agent 021-026ï¼šé¢¨éšªè©•åˆ†ä¸­...")
    progress.progress(85)
    risk_score = min(10.0, 2.0 + stats['æº«åº¦ç•°å¸¸æ‰¹æ¬¡'] * 1.5)
    results["risk_score"] = risk_score
    results["risk_level"] = "ðŸŸ¢ ä½Ž" if risk_score < 4 else "ðŸŸ¡ ä¸­" if risk_score < 7 else "ðŸ”´ é«˜" if risk_score < 9 else "âš« ç·Šæ€¥"

    # Agent 031: æœ€çµ‚å ±å‘Šç”Ÿæˆï¼ˆçœŸæ­£å‘¼å« LLMï¼‰
    status.text("ðŸ“„ Agent 031ï¼šç”Ÿæˆå®Œæ•´å ±å‘Šä¸­...")
    progress.progress(95)

    prompt = f"""
è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šç”Ÿæˆå°ˆæ¥­çš„é£Ÿå“æº¯æºåˆ†æžå ±å‘Šï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š

è³‡æ–™æ‘˜è¦ï¼š
{stats}

æº«åº¦ç•°å¸¸æ‰¹æ¬¡ï¼š{df[df['temperature_violation']==True].to_markdown(index=False) if 'temperature_violation' in df.columns else 'ç„¡'}

è«‹åš´æ ¼æŒ‰ç…§è¦ç¯„æ ¼å¼è¼¸å‡ºæœ€çµ‚å ±å‘Šã€‚
"""

    if llm_call:
        try:
            report = llm_call(prompt, model)
        except Exception as e:
            report = f"âš ï¸ LLM å‘¼å«å¤±æ•—ï¼ˆ{e}ï¼‰ï¼Œä»¥ä¸‹ç‚ºæœ¬åœ°åˆ†æžçµæžœï¼š\n\n" + "\n".join(results["notes"])
    else:
        report = "âš ï¸ æœªæä¾› API Keyï¼Œä½¿ç”¨æœ¬åœ°æ¨¡æ“¬å ±å‘Š\n\n" + "\n".join(results["notes"])

    results["final_report"] = report
    progress.progress(100)
    status.text("ðŸŽ‰ æ‰€æœ‰ 31 å€‹ä»£ç†åŸ·è¡Œå®Œç•¢ï¼")
    time.sleep(1)
    progress.empty()
    status.empty()

    return results

# ==================== Streamlit UI ====================
st.set_page_config(page_title="ðŸ” å°ç£è›‹å“æº¯æºAIç³»çµ± v2.0", layout="wide", initial_sidebar_state="expanded")
st.title("ðŸ” é£Ÿå“æº¯æºAIç³»çµ± v2.0")
st.markdown("### 31å€‹å°ˆæ¥­AIä»£ç† Â· å³æ™‚å†·éˆé¢¨éšªè©•ä¼° Â· ä¸€éµç”Ÿæˆé£Ÿå®‰å ±å‘Š")

with st.sidebar:
    st.header("ðŸ”‘ API é‡‘é‘°è¨­å®šï¼ˆä»»é¸ä¸€ï¼‰")
    openai_key = st.text_input("OpenAI (gpt-4o)", type="password", value=os.getenv("OPENAI_API_KEY", ""))
    gemini_key = st.text_input("Google Gemini 1.5 Pro", type="password", value=os.getenv("GEMINI_API_KEY", ""))
    groq_key = st.text_input("Grok / Llama3 (Groq è¶…å¿«)", type="password", value=os.getenv("GROQ_API_KEY", ""))

    st.session_state.openai_key = openai_key
    st.session_state.gemini_key = gemini_key
    st.session_state.groq_key = groq_key

    st.divider()
    st.caption("ðŸš€ éƒ¨ç½²æ–¼ Hugging Face Spaces Â· 2025-11-21 æ›´æ–°")

# ä¸»ç•«é¢
col1, col2 = st.columns([2, 1])
with col1:
    uploaded_file = st.file_uploader(
        "ðŸ“ ä¸Šå‚³è›‹å“æº¯æºè³‡æ–™ï¼ˆCSV / Excel / JSONï¼‰",
        type=["csv", "xlsx", "json"],
        help="æ¬„ä½å»ºè­°åŒ…å«ï¼šbatch_idã€laying_dateã€temperatureã€farm_name ç­‰"
    )

with col2:
    st.markdown("#### ðŸ“‹ ç¯„ä¾‹è³‡æ–™")
    sample_csv = """
batch_id,farm_name,laying_date,packing_date,temperature
BATCH_001,å¿«æ¨‚è¾²å ´,2025-11-01,2025-11-01,4.5
BATCH_002,é™½å…‰è¾²å ´,2025-11-02,2025-11-03,9.2
BATCH_003,ç¶ è‰²ç‰§å ´,2025-11-03,2025-11-03,3.8
    """.strip()
    st.download_button("â¬‡ï¸ ä¸‹è¼‰ç¯„ä¾‹ CSV", sample_csv, "sample_egg_traceability.csv", "text/csv")

if uploaded_file:
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith('.xlsx'):
            df = pd.read_excel(uploaded_file)
        else:
            df = pd.read_json(uploaded_file)

        st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(df):,} ç­†è³‡æ–™ï¼Œå…± {len(df.columns)} æ¬„")
        st.dataframe(df.head(10), use_container_width=True)

        if st.button("ðŸš€ å•Ÿå‹• 31 å€‹ AI ä»£ç†é€²è¡Œå®Œæ•´åˆ†æž", type="primary", use_container_width=True):
            llm_call = get_llm_client()
            with st.spinner("Agent 031 å”èª¿å“¡å·²å°±ä½ï¼Œæ­£åœ¨èª¿åº¦ 31 å€‹å°ˆæ¥­ä»£ç†..."):
                result = run_all_agents(df.copy(), llm_call, "gpt-4o")

            st.success("ðŸŽ‰ åˆ†æžå®Œæˆï¼ä»¥ä¸‹ç‚º AI ç”Ÿæˆå ±å‘Š")

            # é¢¨éšªç¸½è¦½
            col_a, col_b, col_c = st.columns(3)
            col_a.metric("æœ€é«˜é¢¨éšªåˆ†æ•¸", f"{result['risk_score']:.1f}/10")
            col_b.metric("é¢¨éšªç­‰ç´š", result['risk_level'])
            col_c.metric("ç•°å¸¸æ‰¹æ¬¡", len(result.get('figures', {}))

            # åœ–è¡¨
            if "æº«åº¦è¶¨å‹¢" in result["figures"]:
                st.plotly_chart(result["figures"]["æº«åº¦è¶¨å‹¢"], use_container_width=True)

            # æœ€çµ‚å ±å‘Š
            st.markdown("### ðŸ“„ AI å°ˆæ¥­åˆ†æžå ±å‘Š")
            st.markdown(result["final_report"])

            # ä¸‹è¼‰
            st.download_button(
                "â¬‡ï¸ ä¸‹è¼‰å®Œæ•´å ±å‘Š (Markdown)",
                result["final_report"],
                f"è›‹å“æº¯æºå ±å‘Š_{datetime.now().strftime('%Y%m%d')}.md",
                "text/markdown"
            )

    except Exception as e:
        st.error(f"è³‡æ–™è®€å–å¤±æ•—ï¼š{e}")

else:
    st.info("ðŸ‘ˆ è«‹ä¸Šå‚³è³‡æ–™ä¸¦è¨­å®šè‡³å°‘ä¸€å€‹ API Key å³å¯å•Ÿå‹• 31 å€‹ AI ä»£ç†ï¼")
    st.markdown("### ðŸ”¥ æ”¯æ´æ¨¡åž‹ï¼šGPT-4o Â· Gemini 1.5 Pro Â· Grok Â· Llama3-70Bï¼ˆGroq è¶…å¿«ï¼‰")

st.markdown("---")
st.caption("Food Traceability AI System v2.0 - Built with â¤ï¸ by xAI & Taiwan Food Safety Team")
